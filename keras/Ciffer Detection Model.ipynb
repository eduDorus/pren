{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ciffer Detection Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating generators for training and validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we create two image data generators. The training generator takes an image and applies some simple augmentation by the defined transformations in test_datagen(implementation in next section). We implemented some simple zooming, shifting, and rotation. This helps us to get a lot of unique images out of a small dataset of images. Below we provided some generated samples to make clear what the data augmentation does. In addition, we have a test image data generator. This generator simply takes the image and rescales by 1./255. When we validate or test the network we don't want to manipulate real world data.\n",
    "\n",
    "After the generators have been created we serve them images from our directory ../images/ciffer/train and ../images/ciffer/validation. Every subfolder in this directory will be transformed to a feature. Because this network uses a category classifier we can use as much subfolders as we want. In our case we define 6 final features. These features are the ciffers I, II, III, IV, V and empty. The empty feature is important to recognise false images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img\n",
    "\n",
    "batch_size = 196\n",
    "image_height = 256\n",
    "image_width = 256\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=5,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=False,\n",
    "        samplewise_center=False,\n",
    "        samplewise_std_normalization=False,)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255, samplewise_center=False,\n",
    "        samplewise_std_normalization=False,)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        '../images/ciffer/train',  # this is the target directory\n",
    "        target_size=(image_height, image_width),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        color_mode = 'grayscale',\n",
    "        class_mode='categorical')\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        '../images/ciffer/validation',\n",
    "        target_size=(image_height, image_width),\n",
    "        batch_size=batch_size,\n",
    "        color_mode = 'grayscale',\n",
    "        class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmented images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pyplot\n",
    "for X_batch, y_batch in train_generator:\n",
    "# grid of 3x3 images\n",
    "    fig = pyplot.figure(figsize=(20, 10))\n",
    "    \n",
    "    n = 4\n",
    "\n",
    "    for i in range(0, n):\n",
    "        a=fig.add_subplot(1,n,i+1)\n",
    "\n",
    "        img = array_to_img(X_batch[i])\n",
    "        pyplot.imshow(img, cmap=pyplot.get_cmap('gray'))\n",
    "        pyplot.axis('off')\n",
    "    pyplot.show()\n",
    "    break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our Convolutional Neural Network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, we define the layers of our convolutional neural network. The basic of deep learning is stacking a lot of layers on top of each other to create a deep and more robust neural network. We have implemented three convolutional layers with rectifier activation and max pooling. This is a best practice method and used in known nets like VGG16 / VGG19. Afterward, we flatten our filters and add a dense layer(normal neural network layer) with rectifier activation. To prevent our network from overfitting there is a dropout layer and the final dense sigmoid layer to see probability distribution on the possible outcomes. Our model summary shows how many parameters we need for the network. In order to have a good performance on a raspberry pi, we trim the network to the least parameters we could."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, (7, 7), activation='relu', padding='same', name='block1_conv1', input_shape=(image_height, image_width, 1)))\n",
    "model.add(Conv2D(16, (7, 7), activation='relu', padding='same', name='block1_conv2',))\n",
    "model.add(MaxPooling2D(pool_size=(4, 4), strides=(4, 4), name='block1_pool'))\n",
    "\n",
    "model.add(Conv2D(32, (5, 5), activation='relu', padding='same', name='block2_conv1',))\n",
    "model.add(Conv2D(32, (5, 5), activation='relu', padding='same', name='block2_conv2',))\n",
    "model.add(MaxPooling2D(pool_size=(4, 4), strides=(4, 4), name='block2_pool'))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', name='block3_conv1',))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', name='block3_conv2',))\n",
    "model.add(MaxPooling2D(pool_size=(4, 4), strides=(4, 4), name='block3_pool'))\n",
    "\n",
    "# the model so far outputs 3D feature maps (height, width, features)\n",
    "\n",
    "# this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu', name=\"fc-1\"))\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(128, activation='relu', name=\"fc-2\"))\n",
    "model.add(Dense(6, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "callbacks = TensorBoard(log_dir='./logs', histogram_freq=0,  write_graph=True, write_images=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display how many parameters have been used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we feed the data from the generators to our network. Our training procedure contains hundred iterations training and then we validate on real world data that is not used for training. This helps us determine if the network works with images it has not seen before. After training we save our model weights to ../models/ciffer.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=100,\n",
    "        epochs=1,\n",
    "        validation_data=validation_generator,\n",
    "        verbose=1,\n",
    "        validation_steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save our network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('../models/ciffer.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('../models/ciffer.h5', by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network is finally trained and contains all the knowledge to classify new images. In this step, we test the network on its knowledge. All images are new to the network so it can not know them from the past. We make a prediction for every image and will see how good it performs. This is like our final exam to determine if the learning process was successful or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "        '../images/ciffer/test',\n",
    "        target_size=(image_height, image_width),\n",
    "        batch_size=batch_size,\n",
    "        color_mode = 'grayscale',\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate_generator(\n",
    "        test_generator,\n",
    "        steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This score is really bad, but we only let it train for one epoch. The real model will be trained on dedicated hardware(GPU's) and is out of scope for this document."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
